**Prefill Phase**:
  also called as prompt phase/intial context phase
  
  Happens at the start of the request
 
  Model processes to build an **internal state also called as KV CACHE**
  
  No tokens are generated at this phase, it is only preparing the context
  
  Compute Bound operation

**Decode Phase**:
  
  also called as Generation phase/Auto regressive phase
  
  Model begins generating tokens one by one
  
  Each token is generated based on the previous one and stored context
  
  KV cache built in the previous state is reused so this phase is faster
  
  Memory Bound Operation: speed of generation of tokens depends on the retrieval of the K,V from cache

**KV CACHE**:
  
  In attention layers Instead of computing K, V and Q again during decode phase just cache and reuse them
  
  Without KV cache model would have to recompute attention from scratch over entire promt + generated tokens so far

**Lookahead**: 

Tokens are generated by looking at what comes later also, not auto-regressive.

**Deterministic**: 

If the input is same then output also will always be same.

**Diversity**: 

For same input, there will be variations in the output


**Types of Decoding**:

  **Greedy decoding**:
    
    Chooses the token with highest probability at each step
    
    No lookahead
  
  **Beam Search decoding**:
    
    Maintains multiple hypothesis (beam width K) at each step.
    
    For Partial sentence it **considers top-k next tokens**
    
    Evaluates paths several steps ahead so it is lookahead decoding
  
  **Top-K Sampling**:
    
    Samples from the top-k most probabale next tokens instead of picking the highest
    
    Not deterministic and No Lookahead
  
  **Top-P Sampling**:
    
    Similar to top-k, but instead of fixed k, it samples from the smallest set of tokens whose cumulative probability is >=p 
    No lookahead
  
  **Temperature Sampling**:
    
    Applies probability to the probability to the probability distribution
    
    High temperature(>1) -> more randomness
    
    Low temperature(<1) -> more confident/predictable
    
    No lookahead
  
  **Parallel Sampling**
  
  **Contrastive Decoding**:
  
  **Speculative Decoding**:
 
    
  
  
  
